\begin{thebibliography}{23}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal et~al.(2023)Agrawal, Panwar, Mohan, Kwatra, Gulavani, and Ramjee]{chunked_prefill}
Agrawal, A., Panwar, A., Mohan, J., Kwatra, N., Gulavani, B.~S., and Ramjee, R.
\newblock Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills.
\newblock \emph{ArXiv}, abs/2308.16369, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:261395577}.

\bibitem[Agrawal et~al.(2024)Agrawal, Kedia, Panwar, Mohan, Kwatra, Gulavani, Tumanov, and Ramjee]{chunked_prefill2}
Agrawal, A., Kedia, N., Panwar, A., Mohan, J., Kwatra, N., Gulavani, B.~S., Tumanov, A., and Ramjee, R.
\newblock Taming throughput-latency tradeoff in llm inference with sarathi-serve.
\newblock In \emph{USENIX Symposium on Operating Systems Design and Implementation}, 2024.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:268249103}.

\bibitem[Chen et~al.(2023)Chen, Zaharia, and Zou]{chen2023chatgptsbehaviorchangingtime}
Chen, L., Zaharia, M., and Zou, J.
\newblock How is chatgpt's behavior changing over time?, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.09009}.

\bibitem[Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and Zhou]{ultrachat2023}
Ding, N., Chen, Y., Xu, B., Qin, Y., Zheng, Z., Hu, S., Liu, Z., Sun, M., and Zhou, B.
\newblock Enhancing chat language models by scaling high-quality instructional conversations.
\newblock \emph{arXiv preprint arXiv:2305.14233}, 2023.

\bibitem[{Gemma Team} et~al.(2024){Gemma Team}, Riviere, Pathak, Sessa, Hardin, Bhupatiraju, Hussenot, Mesnard, Shahriari, Ramé, Ferret, Liu, Tafti, Friesen, Casbon, et~al.]{gemma2}
{Gemma Team}, Riviere, M., Pathak, S., Sessa, P.~G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Ramé, A., Ferret, J., Liu, P., Tafti, P., Friesen, A., Casbon, M., et~al.
\newblock Gemma 2: Improving open language models at a practical size, 2024.
\newblock URL \url{https://arxiv.org/abs/2408.00118}.

\bibitem[Ghodsi et~al.(2017)Ghodsi, Gu, and Garg]{safetynets}
Ghodsi, Z., Gu, T., and Garg, S.
\newblock Safetynets: Verifiable execution of deep neural networks on an untrusted cloud, 2017.
\newblock URL \url{https://arxiv.org/abs/1706.10268}.

\bibitem[Goldberg(1991)]{goldberg_fperror}
Goldberg, D.
\newblock What every computer scientist should know about floating-point arithmetic.
\newblock \emph{ACM Comput. Surv.}, 23\penalty0 (1):\penalty0 5–48, March 1991.
\newblock ISSN 0360-0300.
\newblock \doi{10.1145/103162.103163}.
\newblock URL \url{https://doi.org/10.1145/103162.103163}.

\bibitem[Golden et~al.(2024)Golden, Hsia, Sun, Acun, Hosmer, Lee, DeVito, Johnson, Wei, Brooks, and Wu]{golden2024flashattentionstable}
Golden, A., Hsia, S., Sun, F., Acun, B., Hosmer, B., Lee, Y., DeVito, Z., Johnson, J., Wei, G.-Y., Brooks, D., and Wu, C.-J.
\newblock Is flash attention stable?, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.02803}.

\bibitem[Grattafiori et~al.(2024)Grattafiori, Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Vaughan, Yang, Fan, et~al.]{llama3}
Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., et~al.
\newblock The llama 3 herd of models, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.21783}.

\bibitem[Jaghouar et~al.(2024)Jaghouar, Ong, Basra, Obeid, Straube, Keiblinger, Bakouch, Atkins, Panahi, Goddard, Ryabinin, and Hagemann]{intellect1}
Jaghouar, S., Ong, J.~M., Basra, M., Obeid, F., Straube, J., Keiblinger, M., Bakouch, E., Atkins, L., Panahi, M., Goddard, C., Ryabinin, M., and Hagemann, J.
\newblock Intellect-1 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2412.01152}.

\bibitem[Kalamkar et~al.(2019)Kalamkar, Mudigere, Mellempudi, Das, Banerjee, Avancha, Vooturi, Jammalamadaka, Huang, Yuen, Yang, Park, Heinecke, Georganas, Srinivasan, Kundu, Smelyanskiy, Kaul, and Dubey]{kalamkar2019studybfloat16deeplearning}
Kalamkar, D., Mudigere, D., Mellempudi, N., Das, D., Banerjee, K., Avancha, S., Vooturi, D.~T., Jammalamadaka, N., Huang, J., Yuen, H., Yang, J., Park, J., Heinecke, A., Georganas, E., Srinivasan, S., Kundu, A., Smelyanskiy, M., Kaul, B., and Dubey, P.
\newblock A study of bfloat16 for deep learning training, 2019.
\newblock URL \url{https://arxiv.org/abs/1905.12322}.

\bibitem[Kang et~al.(2022)Kang, Hashimoto, Stoica, and Sun]{zkDNN}
Kang, D., Hashimoto, T., Stoica, I., and Sun, Y.
\newblock Scaling up trustless dnn inference with zero-knowledge proofs, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.08674}.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica]{vLLM}
Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.~H., Gonzalez, J.~E., Zhang, H., and Stoica, I.
\newblock Efficient memory management for large language model serving with pagedattention.
\newblock In \emph{Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles}, 2023.

\bibitem[Micikevicius et~al.(2022)Micikevicius, Stosic, Burgess, Cornea, Dubey, Grisenthwaite, Ha, Heinecke, Judd, Kamalu, Mellempudi, Oberman, Shoeybi, Siu, and Wu]{micikevicius2022fp8formatsdeeplearning}
Micikevicius, P., Stosic, D., Burgess, N., Cornea, M., Dubey, P., Grisenthwaite, R., Ha, S., Heinecke, A., Judd, P., Kamalu, J., Mellempudi, N., Oberman, S., Shoeybi, M., Siu, M., and Wu, H.
\newblock Fp8 formats for deep learning, 2022.
\newblock URL \url{https://arxiv.org/abs/2209.05433}.

\bibitem[{Modulus Labs}(2023)]{firstZKML}
{Modulus Labs}.
\newblock The cost of intelligence: Proving machine learning inference with zero-knowledge.
\newblock 2023.

\bibitem[Monniaux(2008)]{pitfalls_of_fp_verification}
Monniaux, D.
\newblock The pitfalls of verifying floating-point computations.
\newblock \emph{ACM Trans. Program. Lang. Syst.}, 30\penalty0 (3), May 2008.
\newblock ISSN 0164-0925.
\newblock \doi{10.1145/1353445.1353446}.
\newblock URL \url{https://doi.org/10.1145/1353445.1353446}.

\bibitem[Shi et~al.(2024)Shi, Zhang, Yao, Li, and Zhao]{shi2024costdownreviewmethods}
Shi, L., Zhang, H., Yao, Y., Li, Z., and Zhao, H.
\newblock Keep the cost down: A review on methods to optimize llm' s kv-cache consumption, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.18003}.

\bibitem[Sun et~al.(2023)Sun, Bai, Li, and Zhang]{zkDL}
Sun, H., Bai, T., Li, J., and Zhang, H.
\newblock zkdl: Efficient zero-knowledge proofs of deep learning training, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.16273}.

\bibitem[Sun et~al.(2024{\natexlab{a}})Sun, Li, and Zhang]{zkllm}
Sun, H., Li, J., and Zhang, H.
\newblock zkllm: Zero knowledge proofs for large language models, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2404.16109}.

\bibitem[Sun et~al.(2024{\natexlab{b}})Sun, Li, Zhang, Jin, and Zhang]{svip}
Sun, Y., Li, Y., Zhang, Y., Jin, Y., and Zhang, H.
\newblock Svip: Towards verifiable inference of open-source large language models.
\newblock \emph{arXiv preprint arXiv:2410.22307}, 2024{\natexlab{b}}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{attention_is_all_you_need}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Proceedings of the 31st International Conference on Neural Information Processing Systems}, NIPS'17, pp.\  6000–6010, Red Hook, NY, USA, 2017. Curran Associates Inc.
\newblock ISBN 9781510860964.

\bibitem[Whitehead \& Fit-Florea(2011)Whitehead and Fit-Florea]{ieee754_nvidia}
Whitehead, N. and Fit-Florea, A.
\newblock Precision and performance: Floating point and ieee 754 compliance for nvidia gpus.
\newblock 2011.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:9720680}.

\bibitem[Zhang et~al.(2024)Zhang, Wang, Dhamankar, Fredrikson, and Agarwal]{Verisplit}
Zhang, H., Wang, Z., Dhamankar, M., Fredrikson, M., and Agarwal, Y.
\newblock Verisplit: Secure and practical offloading of machine learning inferences across iot devices.
\newblock \emph{arXiv preprint arXiv:2406.00586}, 2024.

\end{thebibliography}
